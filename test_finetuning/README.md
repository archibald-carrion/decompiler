# ğŸ›ï¸ Roman Empire Fine-tuning with Docker

This Docker setup automatically handles all dependencies, training, and cleanup for fine-tuning DistilGPT2 on Roman Empire content.

## ğŸ“‹ Prerequisites

- Docker installed and running
- The `roman_empire_finetune.py` script in the same directory

## ğŸš€ Quick Start

### Method 1: Using Docker Compose

1. **Train the model:**
   ```bash
   docker-compose run --rm roman-empire-training python roman_empire_finetune.py --train
   ```

2. **Interactive mode:**
   ```bash
   docker-compose run --rm roman-empire-training python roman_empire_finetune.py --interactive
   ```

### Method 2: Direct Docker Commands

1. **Build the image:**
   ```bash
   docker build -t roman-empire-finetuning .
   ```

2. **Train the model:**
   ```bash
   docker run -it --rm -v $(pwd)/models:/app/models roman-empire-finetuning python roman_empire_finetune.py --train
   ```

## ğŸ“ File Structure

```
test_finetuning/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ cache
â”œâ”€â”€ data
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ execution_error.txt
â”œâ”€â”€ models
â”‚   â”œâ”€â”€ distilgpt2
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ generation_config.json
â”‚   â”‚   â”œâ”€â”€ merges.txt
â”‚   â”‚   â”œâ”€â”€ model.safetensors
â”‚   â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚   â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”‚   â””â”€â”€ vocab.json
â”‚   â””â”€â”€ distilgpt2-roman-empire
â”‚       â”œâ”€â”€ checkpoint-30
â”‚       â”‚   â”œâ”€â”€ config.json
â”‚       â”‚   â”œâ”€â”€ generation_config.json
â”‚       â”‚   â”œâ”€â”€ merges.txt
â”‚       â”‚   â”œâ”€â”€ model.safetensors
â”‚       â”‚   â”œâ”€â”€ optimizer.pt
â”‚       â”‚   â”œâ”€â”€ rng_state.pth
â”‚       â”‚   â”œâ”€â”€ scheduler.pt
â”‚       â”‚   â”œâ”€â”€ special_tokens_map.json
â”‚       â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚       â”‚   â”œâ”€â”€ trainer_state.json
â”‚       â”‚   â”œâ”€â”€ training_args.bin
â”‚       â”‚   â””â”€â”€ vocab.json
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ merges.txt
â”‚       â”œâ”€â”€ model.safetensors
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â”œâ”€â”€ training_args.bin
â”‚       â”œâ”€â”€ training_info.json
â”‚       â””â”€â”€ vocab.json
â”œâ”€â”€ prompt_model_comparison.py
â”œâ”€â”€ prompt_poc_post_tuning.py
â”œâ”€â”€ prompt_poc_pre_tuning.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ roman_empire_finetune.py
â””â”€â”€ run_roman_training.sh
```

## Model Usage
### Testing the prompt of the fine-tuned model
```bash
python prompt_poc_post_tuning.py --prompt "The Roman empire is"
```
### Testing the prompt of the base model
```bash
python prompt_poc_pre_tuning.py --prompt "The Roman empire is"
```
### Testing both models
Using the `prompt_model_comparison.py` script, you can compare the outputs of both the base and fine-tuned models for a set of prompts.
```bash
python prompt_model_comparison.py --prompt-file prompts.txt --output-json results.json
```
Or for a single prompt:
```bash
python prompt_model_comparison.py --prompt "The Roman Empire was" --output-json results.json
```

## ğŸ’¾ Model Persistence

- **Models are saved to:** `./models/` directory on your host machine
- **Survives container deletion:** Yes, models persist between runs
- **Cache location:** `./cache/` directory (speeds up subsequent runs)
- 
---

**Note:** The Docker container automatically deletes itself after each run, but your trained models remain safely stored in the `./models/` directory!